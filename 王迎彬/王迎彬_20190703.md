# Ceph介绍、原理、架构
## 1. Ceph架构简介及使用场景介绍
### 1.1 Ceph简介
Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。

Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。

### 1.2 Ceph特点

高性能:

a. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。

b.考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。

c. 能够支持上千个存储节点的规模，支持TB到PB级的数据。

高可用性:

a. 副本数可以灵活控制。

b. 支持故障域分隔，数据强一致性。

c. 多种故障场景自动进行修复自愈。

d. 没有单点故障，自动管理。

高可扩展性:

a. 去中心化。

b. 扩展灵活。

c. 随着节点增加而线性增长。

特性丰富:

a. 支持三种存储接口：块存储、文件存储、对象存储。

b. 支持自定义接口，支持多种语言驱动。

### 1.3 Ceph架构

支持三种接口：

Object：有原生的API，而且也兼容Swift和S3的API。

Block：支持精简配置、快照、克隆。

File：Posix接口，支持快照。

### 1.4 Ceph核心组件及概念介绍
Monitor:一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。

OSD:OSD全称Object Storage Device，也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。

MDS:MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。

Object:Ceph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。

PG:PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。

RADOS:RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。

Libradio:Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。

CRUSH:CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。

RBD:RBD全称RADOS block device，是Ceph对外提供的块设备服务。

RGW:RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。

CephFS:CephFS全称Ceph File System，是Ceph对外提供的文件系统服务。

### 1.5 三种存储类型-块存储
rbd

典型设备： 磁盘阵列，硬盘

主要是将裸磁盘空间映射给主机使用的。

优点：通过Raid与LVM等手段，对数据提供了保护。多块廉价的硬盘组合起来，提高容量。多块磁盘组合出来的逻辑盘，提升读写效率。

缺点：采用SAN架构组网时，光纤交换机，造价成本高。主机之间无法共享数据。

使用场景：docker容器、虚拟机磁盘存储分配、日志存储、文件存储。
…
### 1.6 三种存储类型-文件存储
fs

典型设备： FTP、NFS服务器

为了克服块存储文件无法共享的问题，所以有了文件存储。在服务器上架设FTP与NFS服务，就是文件存储。

优点：

造价低，随便一台机器就可以了。方便文件共享。

缺点：

读写速率低、传输速率慢。

使用场景：日志存储、有目录结构的文件存储。
…
### 1.7 三种存储类型-对象存储
rgw

典型设备： 内置大容量硬盘的分布式服务器(swift, s3)
多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。

优点：

具备块存储的读写高速、具备文件存储的共享等特性。

使用场景： (适合更新变动较少的数据)

图片存储、视频存储。
